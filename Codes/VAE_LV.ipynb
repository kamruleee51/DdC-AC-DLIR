{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dbbc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the `set_determinism` utility to ensure reproducibility of results\n",
    "from monai.utils import set_determinism, first\n",
    "\n",
    "# Import necessary components for handling data in MONAI\n",
    "from monai.data import DataLoader, Dataset, CacheDataset\n",
    "\n",
    "# Import PyTorch's functional API for implementing layers and functions\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import Structural Similarity Index (SSIM) from the piqa library for image quality assessment\n",
    "from piqa import SSIM\n",
    "\n",
    "# Import pandas for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Import NumPy for numerical operations and PyTorch for deep learning tasks\n",
    "import numpy as np\n",
    "import torch, torchinfo, torchvision\n",
    "\n",
    "# Import `glob` for file pattern matching and `cv2` for image processing\n",
    "from glob import glob\n",
    "import cv2\n",
    "\n",
    "# Import `torch.nn` for constructing neural networks\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Import various loss functions and metrics from MONAI for medical image analysis\n",
    "from monai.losses import BendingEnergyLoss, MultiScaleLoss, DiceLoss\n",
    "from monai.metrics import *\n",
    "\n",
    "# Import `pyplot` from matplotlib for data visualization\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Set determinism for reproducibility\n",
    "# Uncomment the following line if reproducibility is required:\n",
    "# set_determinism(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51238d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of available GPUs\n",
    "print('How many GPUs = ' + str(torch.cuda.device_count()))\n",
    "\n",
    "# Check if a GPU is available and set the device to GPU if available, otherwise fall back to CPU\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)  # Output the device being used (either GPU or CPU)\n",
    "\n",
    "# Raise an exception if no GPU is available, since CPU training can be very slow\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"GPU not available. CPU training will be too slow.\")\n",
    "\n",
    "# Print the name of the GPU being used\n",
    "print(\"device name\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd9b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the image size for the model input\n",
    "img_size = 256\n",
    "\n",
    "# Set the number of training epochs\n",
    "epoch = 50\n",
    "\n",
    "# Set the number of worker threads to load data in parallel\n",
    "num_workers = 8\n",
    "\n",
    "# Flag to indicate whether to use a pre-trained model (0 means no pre-trained model is used)\n",
    "preTrained = 0\n",
    "\n",
    "# File path pattern for training images\n",
    "TrImage = \"data_VAE/aug_train/*.png\"\n",
    "\n",
    "# File path pattern for validation images\n",
    "ValImage = \"data_VAE/aug_val/*.png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154d8b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class for handling images with optional label-to-zero conversion\n",
    "class EchoDatasetMask(Dataset):\n",
    "    def __init__(self, images_path, labtozero):\n",
    "        # Store the paths to the images and the label-to-zero value\n",
    "        self.images_path = images_path\n",
    "        self.labtozero = labtozero\n",
    "        self.n_samples = len(images_path)  # Number of samples in the dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Method to retrieve a single image based on the index\"\"\"\n",
    "        # Read the image from the file path as a grayscale image\n",
    "        image = cv2.imread(self.images_path[index], cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # If labtozero is specified, set all pixels with this label to zero\n",
    "        if self.labtozero is not None:\n",
    "            image[image == self.labtozero] = 0\n",
    "            \n",
    "        # Resize the image to the specified size\n",
    "        image = cv2.resize(image, (img_size, img_size), interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        # Normalize the image by dividing by the maximum pixel value\n",
    "        image = image / (image.max())\n",
    "        \n",
    "        # Expand the dimensions of the image to add a channel dimension\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        \n",
    "        # Convert the image to a float32 type\n",
    "        image = image.astype(np.float32)\n",
    "        \n",
    "        # Return the processed image\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples in the dataset\n",
    "        return self.n_samples\n",
    "\n",
    "# Function to create data loaders for batches of images\n",
    "def get_batches_mask(_dir,\n",
    "                     _labtozero,\n",
    "                     batch_size,\n",
    "                     num_workers,\n",
    "                     pin_memory):\n",
    "    \n",
    "    # Instantiate the custom dataset with the provided directory and label-to-zero value\n",
    "    _data = EchoDatasetMask(images_path=_dir, labtozero=_labtozero)\n",
    "\n",
    "    # Create a DataLoader for batching, shuffling, and loading the data in parallel\n",
    "    _loader = DataLoader(_data,\n",
    "                         batch_size=batch_size,\n",
    "                         num_workers=num_workers,\n",
    "                         pin_memory=pin_memory,\n",
    "                         shuffle=True)\n",
    "\n",
    "    # Return the DataLoader\n",
    "    return _loader\n",
    "\n",
    "# Print the number of training images found in the specified directory\n",
    "print(len(sorted(glob(TrImage))))\n",
    "\n",
    "# Print the number of validation images found in the specified directory\n",
    "print(len(sorted(glob(ValImage))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1cdbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader for training set masks\n",
    "train_mask_LV = get_batches_mask(_dir=sorted(glob(TrImage)),\n",
    "                                 _labtozero=100,\n",
    "                                 batch_size=128,\n",
    "                                 num_workers=num_workers,\n",
    "                                 pin_memory=True)\n",
    "\n",
    "# Create data loader for validation set masks\n",
    "val_mask_LV = get_batches_mask(_dir=sorted(glob(ValImage)),\n",
    "                               _labtozero=100,\n",
    "                               batch_size=8,\n",
    "                               num_workers=num_workers,\n",
    "                               pin_memory=True)\n",
    "\n",
    "# Store the train and validation data loaders in a dictionary\n",
    "dataloaders = {'_train': train_mask_LV, '_val': val_mask_LV}\n",
    "\n",
    "# Retrieve the first training mask from the data loader\n",
    "_train_mask_ = first(dataloaders[\"_train\"])[0][0]\n",
    "\n",
    "# Retrieve the first validation mask from the data loader\n",
    "_val_mask_ = first(dataloaders[\"_val\"])[0][0]\n",
    "\n",
    "# Print the shape of the first training mask\n",
    "print(f\"_train_mask_ shape: {_train_mask_.shape}\")\n",
    "\n",
    "# Print the shape of the first validation mask\n",
    "print(f\"_val_mask_ shape: {_val_mask_.shape}\")\n",
    "\n",
    "# Print the range and unique values of the first training mask\n",
    "print(f\"_train_mask_ range: {_train_mask_.max()} {_train_mask_.min()} {np.unique(_train_mask_)}\")\n",
    "\n",
    "# Print the range and unique values of the first validation mask\n",
    "print(f\"_val_mask_ range: {_val_mask_.max()} {_val_mask_.min()} {np.unique(_val_mask_)}\")\n",
    "\n",
    "# Visualize the first training and validation masks using matplotlib\n",
    "plt.figure(\"check\", (10, 5))\n",
    "\n",
    "# Plot the first training mask\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train mask example\")\n",
    "plt.imshow(_train_mask_, cmap=\"gray\")\n",
    "plt.axis('off')  # Hide axis for a cleaner visualization\n",
    "\n",
    "# Plot the first validation mask\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val mask example\")\n",
    "plt.imshow(_val_mask_, cmap=\"gray\")\n",
    "plt.axis('off')  # Hide axis for a cleaner visualization\n",
    "\n",
    "# Display the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66cab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader for training set masks with label 200 set to zero\n",
    "train_mask_Myo = get_batches_mask(_dir=sorted(glob(TrImage)),\n",
    "                                  _labtozero=200,\n",
    "                                  batch_size=128,\n",
    "                                  num_workers=num_workers,\n",
    "                                  pin_memory=True)\n",
    "\n",
    "# Create data loader for validation set masks with label 200 set to zero\n",
    "val_mask_Myo = get_batches_mask(_dir=sorted(glob(ValImage)),\n",
    "                                _labtozero=200,\n",
    "                                batch_size=8,\n",
    "                                num_workers=num_workers,\n",
    "                                pin_memory=True)\n",
    "\n",
    "# Store the train and validation data loaders in a dictionary\n",
    "dataloaders = {'_train': train_mask_Myo, '_val': val_mask_Myo}\n",
    "\n",
    "# Retrieve the first training mask from the data loader\n",
    "_train_mask_ = first(dataloaders[\"_train\"])[0][0]\n",
    "\n",
    "# Retrieve the first validation mask from the data loader\n",
    "_val_mask_ = first(dataloaders[\"_val\"])[0][0]\n",
    "\n",
    "# Print the shape of the first training mask\n",
    "print(f\"_train_mask_ shape: {_train_mask_.shape}\")\n",
    "\n",
    "# Print the shape of the first validation mask\n",
    "print(f\"_val_mask_ shape: {_val_mask_.shape}\")\n",
    "\n",
    "# Print the range and unique values of the first training mask\n",
    "print(f\"_train_mask_ range: {_train_mask_.max()} {_train_mask_.min()} {np.unique(_train_mask_)}\")\n",
    "\n",
    "# Print the range and unique values of the first validation mask\n",
    "print(f\"_val_mask_ range: {_val_mask_.max()} {_val_mask_.min()} {np.unique(_val_mask_)}\")\n",
    "\n",
    "# Visualize the first training and validation masks using matplotlib\n",
    "plt.figure(\"check\", (10, 5))\n",
    "\n",
    "# Plot the first training mask\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train mask example\")\n",
    "plt.imshow(_train_mask_, cmap=\"gray\")\n",
    "plt.axis('off')  # Hide axis for a cleaner visualization\n",
    "\n",
    "# Plot the first validation mask\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val mask example\")\n",
    "plt.imshow(_val_mask_, cmap=\"gray\")\n",
    "plt.axis('off')  # Hide axis for a cleaner visualization\n",
    "\n",
    "# Display the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7ce019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store training data loaders for different mask types\n",
    "trainData = {\n",
    "    'Myo': train_mask_Myo,  # Myocardium (Myo) training data loader\n",
    "    'LV': train_mask_LV      # Left Ventricle (LV) training data loader\n",
    "}\n",
    "\n",
    "# Create a dictionary to store validation data loaders for different mask types\n",
    "valData = {\n",
    "    'Myo': val_mask_Myo,  # Myocardium (Myo) validation data loader\n",
    "    'LV': val_mask_LV      # Left Ventricle (LV) validation data loader\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a1c56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Variational Encoder class inheriting from PyTorch's nn.Module\n",
    "class VariationalEncoder(nn.Module):\n",
    "    def __init__(self, latent_dims):  \n",
    "        super(VariationalEncoder, self).__init__()\n",
    "        \n",
    "        # Define convolutional layers with increasing feature maps and stride for downsampling\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3, stride=2, padding=1)  # Input: 1 channel, Output: 8 channels\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3, stride=2, padding=1) # Input: 8 channels, Output: 16 channels\n",
    "        self.batch2 = nn.BatchNorm2d(16)  # Batch normalization for the second conv layer\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, stride=2, padding=1) # Input: 16 channels, Output: 32 channels\n",
    "        \n",
    "        # Linear layers to map the output of the conv layers to the latent space\n",
    "        self.linear1 = nn.Linear(img_size // 8 * img_size // 8 * 32, 128) # Flatten and reduce to 128\n",
    "        self.linear2 = nn.Linear(128, latent_dims)  # Map to the latent mean (mu)\n",
    "        self.linear3 = nn.Linear(128, latent_dims)  # Map to the latent log-variance (sigma)\n",
    "\n",
    "        # Define a standard normal distribution for sampling in the latent space\n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        self.N.loc = self.N.loc.to(device)  # Ensure sampling happens on the GPU\n",
    "        self.N.scale = self.N.scale.to(device)\n",
    "        self.kl = 0  # Initialize the KL divergence term\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)  # Move input to the GPU if available\n",
    "        \n",
    "        # Apply convolutional layers with ReLU activations\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.batch2(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        # Flatten the output from the conv layers to prepare for linear layers\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        \n",
    "        # Apply the first linear layer with ReLU activation\n",
    "        x = F.relu(self.linear1(x))\n",
    "        \n",
    "        # Compute the mean (mu) and log-variance (sigma) for the latent space\n",
    "        mu = self.linear2(x)\n",
    "        sigma = torch.exp(self.linear3(x))\n",
    "        \n",
    "        # Sample from the latent space using the reparameterization trick\n",
    "        z = mu + sigma * self.N.sample(mu.shape)\n",
    "        \n",
    "        # Compute the KL divergence between the approximate posterior and the prior\n",
    "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 0.5).sum()\n",
    "        \n",
    "        # Return the sampled latent vector\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b66cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Decoder class inheriting from PyTorch's nn.Module\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dims):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Linear layers to map the latent space back to the original input size\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            nn.Linear(latent_dims, 128),  # Map latent dimensions to 128 features\n",
    "            nn.ReLU(True),  # Apply ReLU activation\n",
    "            nn.Linear(128, img_size//8 * img_size//8 * 32),  # Map to the flattened size from the encoder\n",
    "            nn.ReLU(True)  # Apply ReLU activation\n",
    "        )\n",
    "\n",
    "        # Unflatten the tensor to match the shape expected by the convolutional layers\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(32, img_size//8, img_size//8))\n",
    "        \n",
    "        # Optional softmax layer (currently commented out)\n",
    "        self.m = nn.Softmax(dim=1)\n",
    "\n",
    "        # Convolutional transpose layers to upsample the feature maps back to the original image size\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),  # First upsampling layer\n",
    "            nn.BatchNorm2d(16),  # Batch normalization\n",
    "            nn.ReLU(True),  # Apply ReLU activation\n",
    "            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),  # Second upsampling layer\n",
    "            nn.BatchNorm2d(8),  # Batch normalization\n",
    "            nn.ReLU(True),  # Apply ReLU activation\n",
    "            nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1)  # Final upsampling layer to output 1 channel\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input through the linear decoder layers\n",
    "        x = self.decoder_lin(x)\n",
    "        \n",
    "        # Reshape the output to match the expected input shape for convolutional layers\n",
    "        x = self.unflatten(x)\n",
    "        \n",
    "        # Pass the reshaped tensor through the convolutional decoder layers\n",
    "        x = self.decoder_conv(x)\n",
    "        \n",
    "        # Apply sigmoid activation to the output to obtain pixel values between 0 and 1\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        # Optional softmax activation (currently commented out)\n",
    "        # x = self.m(x)\n",
    "        \n",
    "        # Return the reconstructed image\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1afdfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Variational Autoencoder (VAE) class inheriting from PyTorch's nn.Module\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        # Initialize the encoder with the given latent dimensions\n",
    "        self.encoder = VariationalEncoder(latent_dims)\n",
    "        \n",
    "        # Initialize the decoder with the same latent dimensions\n",
    "        self.decoder = Decoder(latent_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)  # Move the input to the GPU if available\n",
    "        \n",
    "        # Encode the input image to the latent space\n",
    "        z = self.encoder(x)\n",
    "        \n",
    "        # Decode the latent vector back to the image space\n",
    "        return self.decoder(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ced707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducible results\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define the number of latent dimensions for the Variational Autoencoder\n",
    "d = 8\n",
    "\n",
    "# Initialize the Variational Autoencoder with the specified latent dimensions\n",
    "LV_VAE = VariationalAutoencoder(latent_dims=d)\n",
    "\n",
    "# Move the VAE model to the appropriate device (GPU if available, otherwise CPU)\n",
    "LV_VAE.to(device)\n",
    "\n",
    "# If a pre-trained model is specified, load its state dictionary\n",
    "if preTrained: \n",
    "    # Load the model weights from a file, mapping to the correct device\n",
    "    LV_VAE.load_state_dict(torch.load('Myo_VAE_' + str(img_size) + '_.pth', map_location=device))\n",
    "\n",
    "# Optional: Print a summary of the VAE model's architecture and parameter details\n",
    "# torchinfo.summary(LV_VAE, input_size=(2, 1, 512, 512), depth=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c005b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rate for the optimizer\n",
    "lr = 1e-3 \n",
    "\n",
    "# Initialize the Adam optimizer for the Variational Autoencoder\n",
    "# The optimizer will update the model's parameters based on the gradients computed during backpropagation\n",
    "optim_LV_VAE = torch.optim.Adam(\n",
    "    LV_VAE.parameters(),  # The parameters of the model to optimize\n",
    "    lr=lr,                # Learning rate for the optimizer\n",
    "    weight_decay=1e-5     # Weight decay (L2 regularization) to prevent overfitting\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f968caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom SSIM loss class inheriting from the SSIM class\n",
    "class SSIMLoss(SSIM):\n",
    "    def forward(self, x, y):\n",
    "        # Invert the SSIM loss to match the standard loss formulation (higher is worse)\n",
    "        return 1. - super().forward(x, y)\n",
    "\n",
    "# Initialize the SSIM loss with single channel support and move it to the appropriate device\n",
    "ssim_loss_function = SSIMLoss(n_channels=1).to(device)  # .cuda() if GPU support is needed\n",
    "\n",
    "# Initialize the Dice loss for evaluating segmentation performance\n",
    "dice_loss_function = DiceLoss()\n",
    "\n",
    "# Initialize DiceMetric for tracking performance metrics\n",
    "dice_metric_calculator = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "\n",
    "def train_epoch(model, device, dataloader, optimizer):\n",
    "    \"\"\"\n",
    "    Perform one epoch of training for the VAE model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The Variational Autoencoder model\n",
    "    - device: The device to which the model and data are transferred (GPU or CPU)\n",
    "    - dataloader: DataLoader for fetching training batches\n",
    "    - optimizer: Optimizer for updating model weights\n",
    "\n",
    "    Returns:\n",
    "    - A list of average losses and metrics for the epoch\n",
    "    \"\"\"\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize accumulators for different loss components\n",
    "    total_loss = 0.0\n",
    "    total_dice_loss = 0.0\n",
    "    total_ssim_loss = 0.0\n",
    "    total_kl_divergence = 0.0\n",
    "    total_mse_loss = 0.0\n",
    "    mean_dice_score = 0.0\n",
    "    step = 0\n",
    "    \n",
    "    # Iterate over the batches of the dataloader\n",
    "    for batch_images in dataloader:\n",
    "        # Move batch to the appropriate device\n",
    "        batch_images = batch_images.to(device)\n",
    "        \n",
    "        # Forward pass: Generate reconstructed images\n",
    "        reconstructed_images = model(batch_images)\n",
    "        \n",
    "        # Calculate various loss components\n",
    "        mse_loss = ((batch_images - reconstructed_images)**2).sum()  # Mean squared error loss\n",
    "        dice_loss = dice_loss_function(batch_images, reconstructed_images)  # Dice loss for segmentation accuracy\n",
    "        kl_divergence = model.encoder.kl  # KL divergence from the encoder\n",
    "        ssim_loss = ssim_loss_function(batch_images, reconstructed_images)  # SSIM loss (inverted)\n",
    "\n",
    "        # Total loss is the sum of individual losses\n",
    "        total_loss_value = kl_divergence + mse_loss + dice_loss + ssim_loss\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        optimizer.zero_grad()\n",
    "        total_loss_value.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate losses for reporting\n",
    "        total_loss += total_loss_value.item()\n",
    "        total_dice_loss += dice_loss.item()\n",
    "        total_ssim_loss += ssim_loss.item()\n",
    "        total_kl_divergence += kl_divergence.item()\n",
    "        total_mse_loss += mse_loss.item()\n",
    "        mean_dice_score += 1-dice_loss.item()\n",
    "        step+=1\n",
    "\n",
    "\n",
    "    # Calculate average losses over the epoch\n",
    "    num_batches = len(dataloader.dataset)\n",
    "\n",
    "    # Compute and print mean Dice score for the epoch\n",
    "    print(f\"Train mean dice: {(mean_dice_score/step):.4f}\")\n",
    "    \n",
    "    \n",
    "    return [\n",
    "        total_loss / num_batches,\n",
    "        total_dice_loss / num_batches,\n",
    "        total_ssim_loss / num_batches,\n",
    "        total_kl_divergence / num_batches,\n",
    "        total_mse_loss / num_batches,\n",
    "        mean_dice_score / step\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a7177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(model, device, dataloader):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The Variational Autoencoder model\n",
    "    - device: The device to which the model and data are transferred (GPU or CPU)\n",
    "    - dataloader: DataLoader for fetching validation batches\n",
    "\n",
    "    Returns:\n",
    "    - A list of average losses and metrics for the epoch\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode (disables dropout and batch normalization)\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize accumulators for different loss components\n",
    "    total_loss = 0.0\n",
    "    total_dice_loss = 0.0\n",
    "    total_ssim_loss = 0.0\n",
    "    total_kl_divergence = 0.0\n",
    "    total_mse_loss = 0.0\n",
    "    mean_dice_score = 0.0\n",
    "    step =0 \n",
    "    \n",
    "    # Initialize Dice metric calculator\n",
    "    dice_metric_calculator = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for batch_images in dataloader:\n",
    "            # Move batch to the appropriate device\n",
    "            batch_images = batch_images.to(device)\n",
    "            \n",
    "            # Forward pass through the model to get reconstructed images\n",
    "            reconstructed_images = model(batch_images)\n",
    "            \n",
    "            # Calculate various loss components\n",
    "            mse_loss = ((batch_images - reconstructed_images)**2).sum()  # Mean squared error loss\n",
    "            dice_loss = dice_loss_function(batch_images, reconstructed_images)  # Dice loss for segmentation accuracy\n",
    "            kl_divergence = model.encoder.kl  # KL divergence from the encoder\n",
    "            ssim_loss = ssim_loss_function(batch_images, reconstructed_images)  # SSIM loss (inverted)\n",
    "            \n",
    "            # Total loss is the sum of individual losses\n",
    "            total_loss_value = kl_divergence + mse_loss + dice_loss + ssim_loss\n",
    "            \n",
    "            # Accumulate losses for reporting\n",
    "            total_loss += total_loss_value.item()\n",
    "            total_dice_loss += dice_loss.item()\n",
    "            total_ssim_loss += ssim_loss.item()\n",
    "            total_kl_divergence += kl_divergence.item()\n",
    "            total_mse_loss += mse_loss.item()\n",
    "            mean_dice_score += 1-dice_loss.item()\n",
    "            step+=1\n",
    "            \n",
    "\n",
    "        # Compute and print mean Dice score for the validation set\n",
    "        print(f\"Validation mean dice: {(mean_dice_score/step):.4f}\")\n",
    "        \n",
    "        # Calculate average losses over the epoch\n",
    "        num_samples = len(dataloader.dataset)\n",
    "        return [\n",
    "            total_loss / num_samples,\n",
    "            total_dice_loss / num_samples,\n",
    "            total_ssim_loss / num_samples,\n",
    "            total_kl_divergence / num_samples,\n",
    "            total_mse_loss / num_samples,\n",
    "            mean_dice_score / step\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7c4b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outputs(encoder, decoder, dataloader):\n",
    "    \"\"\"\n",
    "    Visualize original and reconstructed images from the Variational Autoencoder.\n",
    "\n",
    "    Parameters:\n",
    "    - encoder: The encoder part of the Variational Autoencoder\n",
    "    - decoder: The decoder part of the Variational Autoencoder\n",
    "    - dataloader: DataLoader providing batches of images for visualization\n",
    "    \"\"\"\n",
    "    # Set figure size for the plots\n",
    "    plt.figure(figsize=(16, 4.5))\n",
    "    \n",
    "    # Iterate through one batch of images from the dataloader\n",
    "    for batch_images in dataloader:\n",
    "        # Set encoder and decoder to evaluation mode\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        \n",
    "        with torch.no_grad():  # No need to compute gradients\n",
    "            # Move images to the appropriate device and pass through the encoder and decoder\n",
    "            batch_images = batch_images.to(device)\n",
    "            reconstructed_images = decoder(encoder(batch_images))\n",
    "            \n",
    "        # Plot original and reconstructed images\n",
    "        num_images = batch_images.size(0)\n",
    "        for i in range(num_images):\n",
    "            # Plot original images\n",
    "            ax = plt.subplot(2, num_images, i + 1)\n",
    "            plt.imshow(batch_images[i].cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "            if i == num_images // 2:\n",
    "                ax.set_title('Original Images')\n",
    "                \n",
    "            # Plot reconstructed images\n",
    "            ax = plt.subplot(2, num_images, i + 1 + num_images)\n",
    "            plt.imshow(reconstructed_images[i].cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "            if i == num_images // 2:\n",
    "                ax.set_title('Reconstructed Images')\n",
    "                \n",
    "        plt.show()\n",
    "        break  # Only plot the first batch of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133bc2ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Number of epochs for training\n",
    "num_epochs = epoch\n",
    "\n",
    "# Metrics to track during training and validation\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
    "\n",
    "# Lists to store loss values and metrics for each epoch\n",
    "train_losses = {\n",
    "    'VAE': [],\n",
    "    'DSC': [],\n",
    "    'SSIM': [],\n",
    "    'KL': [],\n",
    "    'DiffLoss': []\n",
    "}\n",
    "\n",
    "val_losses = {\n",
    "    'VAE': [],\n",
    "    'DSC': [],\n",
    "    'SSIM': [],\n",
    "    'KL': [],\n",
    "    'DiffLoss': []\n",
    "}\n",
    "\n",
    "train_metrics = []\n",
    "val_metrics = []\n",
    "\n",
    "# Initialize best loss tracker\n",
    "best_dice_score = 0\n",
    "best_epoch = 0\n",
    "\n",
    "# Path to save the best model\n",
    "checkpoint_path = f'LV_VAE_{img_size}_.pth'\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\n<<<----------------------- EPOCH {epoch + 1}/{num_epochs} ------------------------->>>')\n",
    "    print('<<<-------------------------- LV ---------------------------->>>')\n",
    "\n",
    "    # Train for one epoch\n",
    "    train_loss = train_epoch(LV_VAE, device, trainData['LV'], optim_LV_VAE)\n",
    "    val_loss = test_epoch(LV_VAE, device, valData['LV'])\n",
    "   \n",
    "    # Check if the validation Dice score has improved\n",
    "    if val_loss[5] > best_dice_score:\n",
    "        best_epoch = epoch + 1\n",
    "        print(f\"Validation Dice Score improved from {best_dice_score:.4f} to {val_loss[5]:.4f}! Saving the best model as {checkpoint_path}\")\n",
    "        best_dice_score = val_loss[5]\n",
    "        torch.save(LV_VAE.state_dict(), checkpoint_path)\n",
    "    \n",
    "    # Print losses for this epoch\n",
    "    print(f'\\nLV Train Loss: {train_loss[0]:.3f} \\t LV Val Loss: {val_loss[0]:.3f}')\n",
    "    print(f\"Best Validation Dice Score: {best_dice_score:.4f} at Epoch {best_epoch}\")\n",
    "\n",
    "    # Record the losses and metrics\n",
    "    train_losses['VAE'].append(train_loss[0])\n",
    "    train_losses['DSC'].append(train_loss[1])\n",
    "    train_losses['SSIM'].append(train_loss[2])\n",
    "    train_losses['KL'].append(train_loss[3])\n",
    "    train_losses['DiffLoss'].append(train_loss[4])\n",
    "    train_metrics.append(train_loss[5])\n",
    "    \n",
    "    val_losses['VAE'].append(val_loss[0])\n",
    "    val_losses['DSC'].append(val_loss[1])\n",
    "    val_losses['SSIM'].append(val_loss[2])\n",
    "    val_losses['KL'].append(val_loss[3])\n",
    "    val_losses['DiffLoss'].append(val_loss[4])\n",
    "    val_metrics.append(val_loss[5])\n",
    "\n",
    "    # Optionally, visualize some outputs\n",
    "    plot_outputs(LV_VAE.encoder, LV_VAE.decoder, valData['LV'])\n",
    "\n",
    "\n",
    "# Save training and validation metrics to a CSV file\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Train_VAE_Loss': np.array(train_losses['VAE']),\n",
    "    'Train_DSC_Loss': np.array(train_losses['DSC']),\n",
    "    'Train_SSIM_Loss': np.array(train_losses['SSIM']),\n",
    "    'Train_KL_Loss': np.array(train_losses['KL']),\n",
    "    'Train_DiffLoss': np.array(train_losses['DiffLoss']),\n",
    "    'Train_Metric': np.array(train_metrics),\n",
    "    'Val_VAE_Loss': np.array(val_losses['VAE']),\n",
    "    'Val_DSC_Loss': np.array(val_losses['DSC']),\n",
    "    'Val_SSIM_Loss': np.array(val_losses['SSIM']),\n",
    "    'Val_KL_Loss': np.array(val_losses['KL']),\n",
    "    'Val_DiffLoss': np.array(val_losses['DiffLoss']),\n",
    "    'Val_Metric': np.array(val_metrics)\n",
    "})\n",
    "\n",
    "# Save the metrics DataFrame to a CSV file\n",
    "metrics_df.to_csv(f'LV_VAE_{img_size}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2cdcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and save the generator and discriminator loss for the LV VAE model\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Plot Training and Validation VAE Loss\n",
    "plt.subplot(151)\n",
    "plt.plot(train_losses['VAE'], label='Train VAE Loss', color='blue')\n",
    "plt.plot(val_losses['VAE'], label='Validation VAE Loss', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('VAE Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Training and Validation DSC Loss\n",
    "plt.subplot(152)\n",
    "plt.plot(train_losses['DSC'], label='Train DSC Loss', color='blue')\n",
    "plt.plot(val_losses['DSC'], label='Validation DSC Loss', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('DSC Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Training and Validation SSIM Loss\n",
    "plt.subplot(153)\n",
    "plt.plot(train_losses['SSIM'], label='Train SSIM Loss', color='blue')\n",
    "plt.plot(val_losses['SSIM'], label='Validation SSIM Loss', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('SSIM Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Training and Validation KL Divergence Loss\n",
    "plt.subplot(154)\n",
    "plt.plot(train_losses['KL'], label='Train KL Divergence Loss', color='blue')\n",
    "plt.plot(val_losses['KL'], label='Validation KL Divergence Loss', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('KL Divergence Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Training and Validation Difference Loss\n",
    "plt.subplot(155)\n",
    "plt.plot(train_losses['DiffLoss'], label='Train Difference Loss', color='blue')\n",
    "plt.plot(val_losses['DiffLoss'], label='Validation Difference Loss', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Difference Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Adjust layout and save the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'LV_VAE_losses_{img_size}.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "886930dad8d4457679a0ca572c30287744b0c7f7c87d123d0e54902f7e45ac8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
