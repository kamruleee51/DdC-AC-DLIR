{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58654a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MONAI (Medical Open Network for AI) libraries for medical imaging processing and transformations.\n",
    "from monai.utils import set_determinism  # Utility to ensure reproducible results.\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirstD,  # Ensures the input image has channels as the first dimension.\n",
    "    Compose,              # Allows combining multiple transformations.\n",
    "    LoadImageD,           # Loads images from disk and wraps them in a dictionary.\n",
    "    RandRotateD,          # Randomly rotates the image within a specified range.\n",
    "    RandZoomD,            # Randomly zooms into the image within a specified range.\n",
    "    ScaleIntensityRanged  # Scales image intensity to a specified range.\n",
    ")\n",
    "import monai\n",
    "from monai.utils import set_determinism, first\n",
    "from monai.networks.layers import Conv, Norm, Pool, same_padding\n",
    "import torchinfo  # Library for summarizing the PyTorch model architecture.\n",
    "from torchviz import make_dot  # Visualizes the computation graph of a PyTorch model.\n",
    "from monai.data import DataLoader, Dataset, CacheDataset  # Utilities for handling datasets and data loading.\n",
    "from monai.config import print_config  # Prints the MONAI configuration and environment info.\n",
    "from monai.networks.blocks import Warp  # Warp block for applying a displacement field to images.\n",
    "from monai.apps import MedNISTDataset  # Utility for working with the MedNIST dataset.\n",
    "import torch.nn.functional as F  # Functional interface in PyTorch, includes many useful operations like activations.\n",
    "from tqdm import tqdm  # Progress bar library for iterating over large loops.\n",
    "\n",
    "\n",
    "# Core PyTorch imports\n",
    "import torch  # PyTorch core library for building and training neural networks.\n",
    "from torch import nn  # PyTorch module containing neural network components.\n",
    "from collections.abc import Sequence  # Collection utilities for handling sequences.\n",
    "from monai.networks.blocks import (\n",
    "    Warp,                     # Warp block for applying a displacement field to images.\n",
    "    Convolution               # Generic convolution block used in many MONAI network architectures.\n",
    ")\n",
    "from monai.networks.blocks.regunet_block import (\n",
    "    RegistrationDownSampleBlock,  # Block for downsampling in a registration network.\n",
    "    get_conv_block,               # Utility function to get a convolution block.\n",
    "    get_deconv_block              # Utility function to get a deconvolution block.\n",
    ")\n",
    "from monai.networks.utils import meshgrid_ij  # Utility to generate a meshgrid for image coordinates.\n",
    "\n",
    "# General-purpose imports for working with files, images, and metrics\n",
    "import os  # Operating system interface for file handling and paths.\n",
    "import cv2  # OpenCV library for image processing.\n",
    "import torchmetrics  # Metrics library for evaluating PyTorch models.\n",
    "from torch.autograd import Variable  # Enables automatic differentiation for tensor operations.\n",
    "from scipy.spatial.distance import directed_hausdorff  # Computes the directed Hausdorff distance between point clouds.\n",
    "import pandas as pd  # Data manipulation library, useful for handling tabular data.\n",
    "import numpy as np  # Numerical operations on large, multi-dimensional arrays and matrices.\n",
    "import matplotlib.pyplot as plt  # Plotting library for visualizing data.\n",
    "import tempfile  # For creating temporary files and directories.\n",
    "from glob import glob  # Unix-style pathname pattern expansion.\n",
    "from monai.losses import *  # Import all loss functions provided by MONAI.\n",
    "from monai.metrics import *  # Import all metrics provided by MONAI.\n",
    "from piqa import SSIM  # Structural Similarity Index (SSIM) metric from PIQA.\n",
    "\n",
    "# Print MONAI configuration to check the setup.\n",
    "print_config()\n",
    "\n",
    "# Set a fixed seed for reproducibility in data transformations, model training, etc.\n",
    "set_determinism(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8094c459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset directory name.\n",
    "dataset_name = 'CAMUS_EStoED_A2C'\n",
    "\n",
    "# Construct the root directory path for the dataset.\n",
    "dataset_root_dir = f'data/{dataset_name}/'\n",
    "print(f'Root directory: {dataset_root_dir}')\n",
    "\n",
    "# Set batch sizes for training and testing.\n",
    "training_batch_size = 6\n",
    "testing_batch_size = 6\n",
    "\n",
    "# Define the size of images to be processed.\n",
    "image_size = 512\n",
    "\n",
    "# Initialize previous model weights and pre-trained model flag.\n",
    "previous_model_weight_size = 256\n",
    "use_pretrained_model = 0\n",
    "\n",
    "# Define the number of training epochs.\n",
    "num_epochs = 500\n",
    "\n",
    "# Set the number of worker threads for data loading.\n",
    "data_loader_workers = 0\n",
    "\n",
    "# Define the experiment name for saving results.\n",
    "experiment_name = \"DdC-AC-DLIR_A2C\"\n",
    "\n",
    "# Construct the file name for saving results or model checkpoints.\n",
    "checkpoint_file_name = f'{experiment_name}_{dataset_name}_{image_size}_'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a80dee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of available GPUs.\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f'Number of GPUs available: {num_gpus}')\n",
    "\n",
    "# Check and select the device (GPU if available, otherwise CPU).\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Selected device: {device}')\n",
    "\n",
    "# Raise an exception if no GPU is available, indicating that CPU training will be too slow.\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"GPU not available. Training on CPU may be too slow.\")\n",
    "\n",
    "# Print the name of the GPU device.\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    print(f'Device name: {device_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ae71d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataset class for handling grayscale images.\n",
    "class EchoDataset(Dataset):\n",
    "    def __init__(self, image_paths, img_size=image_size):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - image_paths (list of str): List of file paths to the images.\n",
    "        - img_size (int): The size to which each image will be resized.\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.img_size = img_size\n",
    "        self.n_samples = len(image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieve an image from the dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - index (int): The index of the image to retrieve.\n",
    "\n",
    "        Returns:\n",
    "        - image (numpy.ndarray): The processed image as a numpy array.\n",
    "        \"\"\"\n",
    "        # Read the image in grayscale mode.\n",
    "        image = cv2.imread(self.image_paths[index], cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Resize the image to the specified size.\n",
    "        image = cv2.resize(image, (self.img_size, self.img_size))\n",
    "        \n",
    "        # Normalize the image pixel values to the range [0, 1].\n",
    "        image = image / image.max()\n",
    "        \n",
    "        # Expand dimensions to add a channel dimension.\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        \n",
    "        # Convert the image to float32 data type.\n",
    "        image = image.astype(np.float32)\n",
    "        \n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of samples in the dataset.\n",
    "        \n",
    "        Returns:\n",
    "        - int: Number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "# Define a dataset class for handling masks associated with grayscale images.\n",
    "class EchoDatasetMask(Dataset):\n",
    "    def __init__(self, mask_paths, img_size=image_size):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - mask_paths (list of str): List of file paths to the mask images.\n",
    "        - img_size (int): The size to which each mask image will be resized.\n",
    "        \"\"\"\n",
    "        self.mask_paths = mask_paths\n",
    "        self.img_size = img_size\n",
    "        self.n_samples = len(mask_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieve a mask from the dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - index (int): The index of the mask to retrieve.\n",
    "\n",
    "        Returns:\n",
    "        - mask (numpy.ndarray): The processed mask as a numpy array.\n",
    "        \"\"\"\n",
    "        # Read the mask image in grayscale mode.\n",
    "        mask = cv2.imread(self.mask_paths[index], cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Resize the mask to the specified size using nearest neighbor interpolation.\n",
    "        mask = cv2.resize(mask, (self.img_size, self.img_size), interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        # Expand dimensions to add a channel dimension.\n",
    "        mask = np.expand_dims(mask, axis=0)\n",
    "        \n",
    "        # Convert the mask to float32 data type.\n",
    "        mask = mask.astype(np.float32)\n",
    "        \n",
    "        return mask\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of samples in the dataset.\n",
    "        \n",
    "        Returns:\n",
    "        - int: Number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return self.n_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188808c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create data loaders for image datasets.\n",
    "def get_batches(image_paths, batch_size, num_workers, pin_memory):\n",
    "    \"\"\"\n",
    "    Create a DataLoader for the image dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - image_paths (list of str): List of file paths to the images.\n",
    "    - batch_size (int): Number of samples per batch.\n",
    "    - num_workers (int): Number of subprocesses to use for data loading.\n",
    "    - pin_memory (bool): Whether to pin memory for faster data transfer to GPU.\n",
    "\n",
    "    Returns:\n",
    "    - DataLoader: DataLoader object for the image dataset.\n",
    "    \"\"\"\n",
    "    dataset = EchoDataset(image_paths=image_paths, img_size=image_size)\n",
    "    data_loader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            num_workers=num_workers,\n",
    "                            pin_memory=pin_memory,\n",
    "                            shuffle=False)\n",
    "    return data_loader\n",
    "\n",
    "# Function to create data loaders for mask datasets.\n",
    "def get_batches_mask(mask_paths, batch_size, num_workers, pin_memory):\n",
    "    \"\"\"\n",
    "    Create a DataLoader for the mask dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - mask_paths (list of str): List of file paths to the masks.\n",
    "    - batch_size (int): Number of samples per batch.\n",
    "    - num_workers (int): Number of subprocesses to use for data loading.\n",
    "    - pin_memory (bool): Whether to pin memory for faster data transfer to GPU.\n",
    "\n",
    "    Returns:\n",
    "    - DataLoader: DataLoader object for the mask dataset.\n",
    "    \"\"\"\n",
    "    dataset = EchoDatasetMask(mask_paths=mask_paths, img_size=image_size)\n",
    "    data_loader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            num_workers=num_workers,\n",
    "                            pin_memory=pin_memory,\n",
    "                            shuffle=False)\n",
    "    return data_loader\n",
    "\n",
    "# Print the number of training and validation samples for images and masks.\n",
    "print(f'Train Sample numbers (fixed_img) = {len(sorted(glob(os.path.join(dataset_root_dir, \"train/fixed_img/*.png\"))))}')\n",
    "print(f'Train Sample numbers (fixed_msk) = {len(sorted(glob(os.path.join(dataset_root_dir, \"train/fixed_msk/*.png\"))))}')\n",
    "print(f'Train Sample numbers (moving_img) = {len(sorted(glob(os.path.join(dataset_root_dir, \"train/moving_img/*.png\"))))}')\n",
    "print(f'Train Sample numbers (moving_msk) = {len(sorted(glob(os.path.join(dataset_root_dir, \"train/moving_msk/*.png\"))))}')\n",
    "print()\n",
    "print(f'Val Sample numbers (fixed_img) = {len(sorted(glob(os.path.join(dataset_root_dir, \"val/fixed_img/*.png\"))))}')\n",
    "print(f'Val Sample numbers (fixed_msk) = {len(sorted(glob(os.path.join(dataset_root_dir, \"val/fixed_msk/*.png\"))))}')\n",
    "print(f'Val Sample numbers (moving_img) = {len(sorted(glob(os.path.join(dataset_root_dir, \"val/moving_img/*.png\"))))}')\n",
    "print(f'Val Sample numbers (moving_msk) = {len(sorted(glob(os.path.join(dataset_root_dir, \"val/moving_msk/*.png\"))))}')\n",
    "print()\n",
    "\n",
    "# Create data loaders for the training dataset.\n",
    "fixed_train_img_loader = get_batches(\n",
    "    image_paths=sorted(glob(os.path.join(dataset_root_dir, \"train/fixed_img/*\"))),\n",
    "    batch_size=training_batch_size,\n",
    "    num_workers=data_loader_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "fixed_train_mask_loader = get_batches_mask(\n",
    "    mask_paths=sorted(glob(os.path.join(dataset_root_dir, \"train/fixed_msk/*\"))),\n",
    "    batch_size=training_batch_size,\n",
    "    num_workers=data_loader_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "moving_train_img_loader = get_batches(\n",
    "    image_paths=sorted(glob(os.path.join(dataset_root_dir, \"train/moving_img/*\"))),\n",
    "    batch_size=training_batch_size,\n",
    "    num_workers=data_loader_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "moving_train_mask_loader = get_batches_mask(\n",
    "    mask_paths=sorted(glob(os.path.join(dataset_root_dir, \"train/moving_msk/*\"))),\n",
    "    batch_size=training_batch_size,\n",
    "    num_workers=data_loader_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Print data loader objects to verify creation.\n",
    "print(\"Train IMG FIXED Loader:\", fixed_train_img_loader)\n",
    "print(\"Train MSK FIXED Loader:\", fixed_train_mask_loader)\n",
    "print(\"Train IMG Moving Loader:\", moving_train_img_loader)\n",
    "print(\"Train MSK Moving Loader:\", moving_train_mask_loader)\n",
    "\n",
    "# Create data loaders for the validation dataset.\n",
    "fixed_val_img_loader = get_batches(\n",
    "    image_paths=sorted(glob(os.path.join(dataset_root_dir, \"val/fixed_img/*\"))),\n",
    "    batch_size=testing_batch_size,\n",
    "    num_workers=data_loader_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "fixed_val_mask_loader = get_batches_mask(\n",
    "    mask_paths=sorted(glob(os.path.join(dataset_root_dir, \"val/fixed_msk/*\"))),\n",
    "    batch_size=testing_batch_size,\n",
    "    num_workers=data_loader_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "moving_val_img_loader = get_batches(\n",
    "    image_paths=sorted(glob(os.path.join(dataset_root_dir, \"val/moving_img/*\"))),\n",
    "    batch_size=testing_batch_size,\n",
    "    num_workers=data_loader_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "moving_val_mask_loader = get_batches_mask(\n",
    "    mask_paths=sorted(glob(os.path.join(dataset_root_dir, \"val/moving_msk/*\"))),\n",
    "    batch_size=testing_batch_size,\n",
    "    num_workers=data_loader_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Print data loader objects to verify creation.\n",
    "print(\"Val IMG FIXED Loader:\", fixed_val_img_loader)\n",
    "print(\"Val MSK FIXED Loader:\", fixed_val_mask_loader)\n",
    "print(\"Val IMG Moving Loader:\", moving_val_img_loader)\n",
    "print(\"Val MSK Moving Loader:\", moving_val_mask_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7806b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store DataLoader objects for different datasets.\n",
    "dataloaders = {\n",
    "    'fixed_train_img': fixed_train_img_loader,\n",
    "    'fixed_train_msk': fixed_train_mask_loader,\n",
    "    'moving_train_img': moving_train_img_loader,\n",
    "    'moving_train_msk': moving_train_mask_loader,\n",
    "    'fixed_val_img': fixed_val_img_loader,\n",
    "    'fixed_val_msk': fixed_val_mask_loader,\n",
    "    'moving_val_img': moving_val_img_loader,\n",
    "    'moving_val_msk': moving_val_mask_loader\n",
    "}\n",
    "\n",
    "# Example usage: Print the DataLoader objects to verify their creation.\n",
    "for key, loader in dataloaders.items():\n",
    "    print(f\"{key} DataLoader: {loader}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1e5a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract samples from each DataLoader\n",
    "fixed_train_img_sample = first(dataloaders[\"fixed_train_img\"])[0][0]\n",
    "fixed_train_msk_sample = first(dataloaders[\"fixed_train_msk\"])[0][0]\n",
    "moving_train_img_sample = first(dataloaders[\"moving_train_img\"])[0][0]\n",
    "moving_train_msk_sample = first(dataloaders[\"moving_train_msk\"])[0][0]\n",
    "\n",
    "fixed_val_img_sample = first(dataloaders[\"fixed_val_img\"])[0][0]\n",
    "fixed_val_msk_sample = first(dataloaders[\"fixed_val_msk\"])[0][0]\n",
    "moving_val_img_sample = first(dataloaders[\"moving_val_img\"])[0][0]\n",
    "moving_val_msk_sample = first(dataloaders[\"moving_val_msk\"])[0][0]\n",
    "\n",
    "# Print shapes of the samples\n",
    "print(f\"fixed_train_img_sample shape: {fixed_train_img_sample.shape}\")\n",
    "print(f\"fixed_train_msk_sample shape: {fixed_train_msk_sample.shape}\")\n",
    "print(f\"moving_train_img_sample shape: {moving_train_img_sample.shape}\")\n",
    "print(f\"moving_train_msk_sample shape: {moving_train_msk_sample.shape}\")\n",
    "print(f\"fixed_val_img_sample shape: {fixed_val_img_sample.shape}\")\n",
    "print(f\"fixed_val_msk_sample shape: {fixed_val_msk_sample.shape}\")\n",
    "print(f\"moving_val_img_sample shape: {moving_val_img_sample.shape}\")\n",
    "print(f\"moving_val_msk_sample shape: {moving_val_msk_sample.shape}\")\n",
    "\n",
    "# Print range of pixel values\n",
    "print(f\"fixed_train_img_sample Range: {fixed_train_img_sample.max()} {fixed_train_img_sample.min()}\")\n",
    "print(f\"fixed_train_msk_sample Range: {fixed_train_msk_sample.max()} {fixed_train_msk_sample.min()} {np.unique(fixed_train_msk_sample)}\")\n",
    "print(f\"moving_train_img_sample Range: {moving_train_img_sample.max()} {moving_train_img_sample.min()}\")\n",
    "print(f\"moving_train_msk_sample Range: {moving_train_msk_sample.max()} {moving_train_msk_sample.min()} {np.unique(moving_train_msk_sample)}\")\n",
    "print(f\"fixed_val_img_sample Range: {fixed_val_img_sample.max()} {fixed_val_img_sample.min()}\")\n",
    "print(f\"fixed_val_msk_sample Range: {fixed_val_msk_sample.max()} {fixed_val_msk_sample.min()} {np.unique(fixed_val_msk_sample)}\")\n",
    "print(f\"moving_val_img_sample Range: {moving_val_img_sample.max()} {moving_val_img_sample.min()}\")\n",
    "print(f\"moving_val_msk_sample Range: {moving_val_msk_sample.max()} {moving_val_msk_sample.min()} {np.unique(moving_val_msk_sample)}\")\n",
    "\n",
    "# Plot samples in a grid\n",
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "# Fixed training images and masks\n",
    "plt.subplot(2, 4, 1)\n",
    "plt.title(\"fixed_train_img_sample\")\n",
    "plt.imshow(fixed_train_img_sample.squeeze(), cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 4, 2)\n",
    "plt.title(\"fixed_train_msk_sample\")\n",
    "plt.imshow(fixed_train_msk_sample.squeeze(), cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 4, 3)\n",
    "plt.title(\"moving_train_img_sample\")\n",
    "plt.imshow(moving_train_img_sample.squeeze(), cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 4, 4)\n",
    "plt.title(\"moving_train_msk_sample\")\n",
    "plt.imshow(moving_train_msk_sample.squeeze(), cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "# Fixed validation images and masks\n",
    "plt.subplot(2, 4, 5)\n",
    "plt.title(\"fixed_val_img_sample\")\n",
    "plt.imshow(fixed_val_img_sample.squeeze(), cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 4, 6)\n",
    "plt.title(\"fixed_val_msk_sample\")\n",
    "plt.imshow(fixed_val_msk_sample.squeeze(), cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 4, 7)\n",
    "plt.title(\"moving_val_img_sample\")\n",
    "plt.imshow(moving_val_img_sample.squeeze(), cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 4, 8)\n",
    "plt.title(\"moving_val_msk_sample\")\n",
    "plt.imshow(moving_val_msk_sample.squeeze(), cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258dc5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "class RegistrationExtractionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Extraction Block for RegUNet.\n",
    "    Extracts features from specified levels and averages them.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims,\n",
    "        extract_levels,\n",
    "        num_channels,\n",
    "        out_channels,\n",
    "        kernel_initializer,\n",
    "        activation,\n",
    "        mode,\n",
    "        align_corners,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spatial_dims: Number of spatial dimensions.\n",
    "            extract_levels: Levels to extract features from, e.g., [0, 1, 2].\n",
    "            num_channels: Number of channels at each level.\n",
    "            out_channels: Number of output channels.\n",
    "            kernel_initializer: Initializer for kernels.\n",
    "            activation: Activation function.\n",
    "            mode: Interpolation mode for feature map resizing.\n",
    "            align_corners: Whether to align corners during interpolation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.extract_levels = extract_levels\n",
    "        self.max_level = max(extract_levels)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                get_conv_block(\n",
    "                    spatial_dims=spatial_dims,\n",
    "                    in_channels=num_channels[d],\n",
    "                    out_channels=out_channels,\n",
    "                    norm=\"BATCH\",\n",
    "                    act=activation,\n",
    "                    initializer=kernel_initializer,\n",
    "                )\n",
    "                for d in extract_levels\n",
    "            ]\n",
    "        )\n",
    "        self.mode = mode\n",
    "        self.align_corners = align_corners\n",
    "\n",
    "    def forward(self, x: list[torch.Tensor], image_size: list[int]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Decoded features at different levels.\n",
    "            image_size: Desired output image size.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (batch, out_channels, size1, size2, size3), where (size1, size2, size3) = image_size.\n",
    "        \"\"\"\n",
    "        feature_list = [\n",
    "            F.interpolate(\n",
    "                layer(x[self.max_level - level]), size=image_size, mode=self.mode, align_corners=self.align_corners\n",
    "            )\n",
    "            for layer, level in zip(self.layers, self.extract_levels)\n",
    "        ]\n",
    "        out: torch.Tensor = torch.mean(torch.stack(feature_list, dim=0), dim=0)\n",
    "        return out\n",
    "\n",
    "def get_conv_layer(\n",
    "    spatial_dims: int, in_channels: int, out_channels: int, kernel_size: Sequence[int] | int = 3\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Creates a convolutional layer with the given parameters.\n",
    "    \n",
    "    Args:\n",
    "        spatial_dims: Number of spatial dimensions.\n",
    "        in_channels: Number of input channels.\n",
    "        out_channels: Number of output channels.\n",
    "        kernel_size: Size of the kernel.\n",
    "    \n",
    "    Returns:\n",
    "        Convolutional layer module.\n",
    "    \"\"\"\n",
    "    padding = same_padding(kernel_size)\n",
    "    return Convolution(\n",
    "        spatial_dims, in_channels, out_channels, kernel_size=kernel_size, bias=False, conv_only=True, padding=padding\n",
    "    )\n",
    "\n",
    "class RegistrationResidualConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual convolutional block with layer normalization and activation.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, spatial_dims: int, in_channels: int, out_channels: int, num_layers: int = 2, kernel_size: int = 3\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spatial_dims: Number of spatial dimensions.\n",
    "            in_channels: Number of input channels.\n",
    "            out_channels: Number of output channels.\n",
    "            num_layers: Number of layers in the block.\n",
    "            kernel_size: Size of the kernel.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                get_conv_layer(\n",
    "                    spatial_dims=spatial_dims,\n",
    "                    in_channels=in_channels if i == 0 else out_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.norms = nn.ModuleList([Norm[Norm.BATCH, spatial_dims](out_channels) for _ in range(num_layers)])\n",
    "        self.acts = nn.ModuleList([nn.ReLU() for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor with shape (batch, in_channels, insize_1, insize_2, [insize_3]).\n",
    "\n",
    "        Returns:\n",
    "            Output tensor with shape (batch, out_channels, insize_1, insize_2, [insize_3]).\n",
    "        \"\"\"\n",
    "        skip = x\n",
    "        for i, (conv, norm, act) in enumerate(zip(self.layers, self.norms, self.acts)):\n",
    "            x = conv(x)\n",
    "            x = norm(x)\n",
    "            if i == self.num_layers - 1:\n",
    "                # Add skip connection on the last layer\n",
    "                x = x + skip\n",
    "            x = act(x)\n",
    "        return x\n",
    "\n",
    "class RegUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified UNet architecture used in RegUNet for registration tasks.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims,\n",
    "        in_channels,\n",
    "        num_channel_initial,\n",
    "        depth,\n",
    "        out_kernel_initializer,\n",
    "        out_activation,\n",
    "        out_channels,\n",
    "        extract_levels,\n",
    "        encode_kernel_sizes,\n",
    "        pooling=True,\n",
    "        concat_skip=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spatial_dims: Number of spatial dimensions.\n",
    "            in_channels: Number of input channels.\n",
    "            num_channel_initial: Number of initial channels.\n",
    "            depth: Depth of the network.\n",
    "            out_kernel_initializer: Initializer for the output layer.\n",
    "            out_activation: Activation function for the output layer.\n",
    "            out_channels: Number of output channels.\n",
    "            extract_levels: Levels to extract features from.\n",
    "            encode_kernel_sizes: Kernel sizes for encoding.\n",
    "            pooling: Whether to use pooling for down-sampling.\n",
    "            concat_skip: Whether to concatenate skipped connections.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if not extract_levels:\n",
    "            extract_levels = (depth,)\n",
    "        if max(extract_levels) != depth:\n",
    "            raise AssertionError(\"Maximum extraction level must equal depth.\")\n",
    "\n",
    "        self.spatial_dims = spatial_dims\n",
    "        self.in_channels = in_channels\n",
    "        self.num_channel_initial = num_channel_initial\n",
    "        self.depth = depth\n",
    "        self.out_kernel_initializer = out_kernel_initializer\n",
    "        self.out_activation = out_activation\n",
    "        self.out_channels = out_channels\n",
    "        self.extract_levels = extract_levels\n",
    "        self.pooling = pooling\n",
    "        self.concat_skip = concat_skip\n",
    "\n",
    "        if isinstance(encode_kernel_sizes, int):\n",
    "            encode_kernel_sizes = [encode_kernel_sizes] * (self.depth + 1)\n",
    "        if len(encode_kernel_sizes) != self.depth + 1:\n",
    "            raise AssertionError(\"Kernel sizes length must match depth + 1.\")\n",
    "        self.encode_kernel_sizes = encode_kernel_sizes\n",
    "\n",
    "        self.num_channels = [self.num_channel_initial * (2**d) for d in range(self.depth + 1)]\n",
    "        self.min_extract_level = min(self.extract_levels)\n",
    "\n",
    "        # Initialize layers\n",
    "        self.build_layers()\n",
    "\n",
    "    def build_layers(self):\n",
    "        self.build_encode_layers()\n",
    "        self.build_decode_layers()\n",
    "\n",
    "    def build_encode_layers(self):\n",
    "        # Encoding layers\n",
    "        self.encode_convs = nn.ModuleList(\n",
    "            [\n",
    "                self.build_conv_block(\n",
    "                    in_channels=self.in_channels if d == 0 else self.num_channels[d - 1],\n",
    "                    out_channels=self.num_channels[d],\n",
    "                    kernel_size=self.encode_kernel_sizes[d],\n",
    "                )\n",
    "                for d in range(self.depth)\n",
    "            ]\n",
    "        )\n",
    "        self.encode_pools = nn.ModuleList(\n",
    "            [self.build_down_sampling_block(channels=self.num_channels[d]) for d in range(self.depth)]\n",
    "        )\n",
    "        self.bottom_block = self.build_bottom_block(\n",
    "            in_channels=self.num_channels[-2], out_channels=self.num_channels[-1]\n",
    "        )\n",
    "\n",
    "    def build_conv_block(self, in_channels, out_channels, kernel_size):\n",
    "        return nn.Sequential(\n",
    "            get_conv_block(\n",
    "                spatial_dims=self.spatial_dims,\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                act='RELU'\n",
    "            ),\n",
    "            RegistrationResidualConvBlock(\n",
    "                spatial_dims=self.spatial_dims,\n",
    "                in_channels=out_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def build_down_sampling_block(self, channels: int):\n",
    "        return RegistrationDownSampleBlock(spatial_dims=self.spatial_dims, channels=channels, pooling=self.pooling)\n",
    "\n",
    "    def build_bottom_block(self, in_channels: int, out_channels: int):\n",
    "        kernel_size = self.encode_kernel_sizes[self.depth]\n",
    "        return nn.Sequential(\n",
    "            get_conv_block(\n",
    "                spatial_dims=self.spatial_dims,\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                act='RELU'\n",
    "            ),\n",
    "            RegistrationResidualConvBlock(\n",
    "                spatial_dims=self.spatial_dims,\n",
    "                in_channels=out_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def build_decode_layers(self):\n",
    "        self.decode_deconvs = nn.ModuleList(\n",
    "            [\n",
    "                self.build_up_sampling_block(in_channels=self.num_channels[d + 1], out_channels=self.num_channels[d])\n",
    "                for d in range(self.depth - 1, self.min_extract_level - 1, -1)\n",
    "            ]\n",
    "        )\n",
    "        self.decode_convs = nn.ModuleList(\n",
    "            [\n",
    "                self.build_conv_block(\n",
    "                    in_channels=(2 * self.num_channels[d] if self.concat_skip else self.num_channels[d]),\n",
    "                    out_channels=self.num_channels[d],\n",
    "                    kernel_size=3,\n",
    "                )\n",
    "                for d in range(self.depth - 1, self.min_extract_level - 1, -1)\n",
    "            ]\n",
    "        )\n",
    "        self.output_block = self.build_output_block()\n",
    "\n",
    "    def build_up_sampling_block(self, in_channels: int, out_channels: int) -> nn.Module:\n",
    "        return get_deconv_block(spatial_dims=self.spatial_dims, in_channels=in_channels, out_channels=out_channels)\n",
    "\n",
    "    def build_output_block(self) -> nn.Module:\n",
    "        return RegistrationExtractionBlock(\n",
    "            spatial_dims=self.spatial_dims,\n",
    "            extract_levels=self.extract_levels,\n",
    "            num_channels=self.num_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_initializer=self.out_kernel_initializer,\n",
    "            activation=self.out_activation,\n",
    "            mode='bilinear',\n",
    "            align_corners=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor with shape (batch, in_channels, insize_1, insize_2, [insize_3]).\n",
    "\n",
    "        Returns:\n",
    "            Output tensor with shape (batch, out_channels, insize_1, insize_2, [insize_3]).\n",
    "        \"\"\"\n",
    "        image_size = x.shape[2:]\n",
    "        skips = []  # Skipped connections\n",
    "        encoded = x\n",
    "        for encode_conv, encode_pool in zip(self.encode_convs, self.encode_pools):\n",
    "            skip = encode_conv(encoded)\n",
    "            encoded = encode_pool(skip)\n",
    "            skips.append(skip)\n",
    "        decoded = self.bottom_block(encoded)\n",
    "\n",
    "        outs = [decoded]\n",
    "\n",
    "        for i, (decode_deconv, decode_conv) in enumerate(zip(self.decode_deconvs, self.decode_convs)):\n",
    "            decoded = decode_deconv(decoded)\n",
    "            if self.concat_skip:\n",
    "                decoded = torch.cat([decoded, skips[-i - 1]], dim=1)\n",
    "            else:\n",
    "                decoded = decoded + skips[-i - 1]\n",
    "            decoded = decode_conv(decoded)\n",
    "            outs.append(decoded)\n",
    "\n",
    "        out = self.output_block(outs, image_size=image_size)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2773c62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RegUNet class (assuming it's already correctly implemented)\n",
    "# ...\n",
    "\n",
    "# Initialize the RegUNet model with specific hyperparameters\n",
    "model = RegUNet(\n",
    "    spatial_dims=2,            # The model will operate on 2D spatial data (e.g., images)\n",
    "    in_channels=2,             # The number of input channels (e.g., 2 for a dual-channel input like multi-modal images)\n",
    "    num_channel_initial=128,   # The number of channels/filters in the first convolutional layer\n",
    "    depth=5,                   # The number of down-sampling/encoding layers in the U-Net architecture\n",
    "    extract_levels=[5],        # Specifies which levels to extract features from (5 corresponds to the deepest layer here)\n",
    "    out_activation=None,       # No activation function is applied to the output layer (e.g., raw logits are returned)\n",
    "    out_channels=2,            # The number of output channels (e.g., 2 for a segmentation task with 2 classes)\n",
    "    out_kernel_initializer=\"zeros\", # Initialize the output layer weights with zeros\n",
    "    concat_skip=False,         # Whether to concatenate (True) or add (False) skip connections\n",
    "    encode_kernel_sizes=3      # The kernel size for convolutions in the encoding path\n",
    ")\n",
    "\n",
    "# Print a summary of the model architecture using torchinfo\n",
    "torchinfo.summary(model, input_size=(2, 2, image_size, image_size), depth=100)\n",
    "# Summary displays detailed information about each layer, \n",
    "# including input/output sizes, number of parameters, and computational complexity.\n",
    "# 'input_size' defines the expected size of the input tensor (batch_size, channels, height, width).\n",
    "# 'depth=100' controls the level of detail shown in the summary (with 100, all layers are included).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c0b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to the specified device (e.g., GPU or CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "# If using a pretrained model, load its state dictionary from a file\n",
    "if use_pretrained_model:\n",
    "    # Construct the path to the pretrained weights file using root_dir, ExpName, dataDir, and previousWeight\n",
    "    model.load_state_dict(torch.load(os.path.join(root_dir, ExpName + '_' + dataDir + '_' + str(previousWeight) + '_'+'.pth')))\n",
    "\n",
    "# Initialize a Warp layer for image transformation with bilinear interpolation and zero-padding\n",
    "warp_layer = Warp(mode='bilinear', padding_mode='zeros').to(device)\n",
    "\n",
    "# Define the image loss function using Global Mutual Information\n",
    "# This loss function measures the mutual information between images\n",
    "image_loss = GlobalMutualInformationLoss()\n",
    "\n",
    "# Define a custom SSIM loss class inheriting from SSIM\n",
    "class SSIMLoss(SSIM):\n",
    "    # Override the forward method to compute the SSIM loss as 1 minus the SSIM score\n",
    "    def forward(self, x, y):\n",
    "        return 1. - super().forward(x, y)\n",
    "    \n",
    "# Instantiate the SSIMLoss with 1 channel (e.g., grayscale images) and move it to the device\n",
    "label_SSIM = SSIMLoss(n_channels=1).to(device)  # Use .cuda() if GPU support is needed\n",
    "\n",
    "# Define the label loss function using Dice Loss\n",
    "label_loss = DiceLoss()\n",
    "\n",
    "# Optionally, you can use a MultiScaleLoss that combines Dice Loss across different scales\n",
    "# label_loss = MultiScaleLoss(label_loss, scales=[0, 1, 2, 4, 8, 16])\n",
    "\n",
    "# Define the regularization term using Bending Energy Loss\n",
    "regularization = BendingEnergyLoss()\n",
    "\n",
    "# Initialize the Adam optimizer with a learning rate of 0.0002 for the model's parameters\n",
    "optimizer = torch.optim.Adam(model.parameters(), 0.0002)\n",
    "\n",
    "# Optionally, define a learning rate scheduler that reduces the learning rate by a factor of 0.5 every 8 epochs\n",
    "# exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.5, verbose=True)\n",
    "\n",
    "# Define a metric for evaluating Dice score, including the background class and computing the mean\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
    "\n",
    "# Optionally, you can compute the mean Dice score directly if you have predictions and ground truth\n",
    "# dice_metric = compute_meandice(y_pred, y, include_background=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225f00ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot(labels, device, C=2):\n",
    "    '''\n",
    "    Converts an integer label tensor to a one-hot encoded tensor.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : torch.Tensor\n",
    "        A tensor of shape (N, 1, H, W) where N is batch size, \n",
    "        and each value is an integer representing the correct classification.\n",
    "    device : torch.device\n",
    "        The device to which the tensor should be moved (e.g., 'cpu' or 'cuda').\n",
    "    C : int\n",
    "        The number of classes in the labels.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    target : torch.Tensor\n",
    "        A tensor of shape (N, C, H, W) where C is the number of classes.\n",
    "        Each element is one-hot encoded, indicating the class label.\n",
    "    '''\n",
    "    # Convert labels to long type tensor (integer type)\n",
    "    labels = labels.long()\n",
    "    \n",
    "    # Create a tensor of shape (N, C, H, W) filled with zeros\n",
    "    one_hot = torch.FloatTensor(labels.size(0), C, labels.size(2), labels.size(3)).zero_().to(device)\n",
    "    \n",
    "    # Use scatter_ to set the appropriate index for each label to 1\n",
    "    # '1' is placed in the channel corresponding to the label value\n",
    "    target = one_hot.scatter_(1, labels.data, 1)\n",
    "    \n",
    "    # Wrap the tensor in a Variable (for backward compatibility; can be omitted if not needed)\n",
    "    target = Variable(target)\n",
    "        \n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814874b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalEncoder(nn.Module):\n",
    "    def __init__(self, latent_dims):  \n",
    "        super(VariationalEncoder, self).__init__()\n",
    "        \n",
    "        # Define the convolutional layers for encoding\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3, stride=2, padding=1)  # First conv layer: input channels=1, output channels=8\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3, stride=2, padding=1) # Second conv layer: input channels=8, output channels=16\n",
    "        self.batch2 = nn.BatchNorm2d(16)  # Batch normalization after second conv layer\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, stride=2, padding=1) # Third conv layer: input channels=16, output channels=32\n",
    "        \n",
    "        # Define linear layers for mapping to latent space\n",
    "        self.linear1 = nn.Linear(image_size//8 * image_size//8 * 32, 128)  # Linear layer to reduce dimensions to 128\n",
    "        self.linear2 = nn.Linear(128, latent_dims)  # Linear layer to output mean of latent space\n",
    "        self.linear3 = nn.Linear(128, latent_dims)  # Linear layer to output log-variance of latent space\n",
    "\n",
    "        # Define a Normal distribution for sampling in latent space\n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        self.N.loc = self.N.loc.to(device)  # Move distribution parameters to the device (GPU/CPU)\n",
    "        self.N.scale = self.N.scale.to(device)\n",
    "        self.kl = 0  # Initialize KL divergence\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)  # Move input data to the device\n",
    "        x = F.relu(self.conv1(x))  # Apply first convolutional layer followed by ReLU activation\n",
    "        x = F.relu(self.batch2(self.conv2(x)))  # Apply second convolutional layer, batch normalization, and ReLU activation\n",
    "        x = F.relu(self.conv3(x))  # Apply third convolutional layer followed by ReLU activation\n",
    "        x = torch.flatten(x, start_dim=1)  # Flatten the tensor to feed into linear layers\n",
    "        x = F.relu(self.linear1(x))  # Apply linear layer followed by ReLU activation\n",
    "        mu = self.linear2(x)  # Compute the mean of the latent space distribution\n",
    "        sigma = torch.exp(self.linear3(x))  # Compute the standard deviation of the latent space distribution\n",
    "        z = mu + sigma * self.N.sample(mu.shape)  # Sample from the latent space using reparameterization trick\n",
    "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()  # Compute the KL divergence\n",
    "        return z  # Return the latent space sample\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define linear layers for decoding\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            nn.Linear(latent_dims, 128),  # Linear layer to expand latent dimensions\n",
    "            nn.ReLU(True),  # ReLU activation\n",
    "            nn.Linear(128, image_size//8 * image_size//8 * 32),  # Linear layer to map to the target size\n",
    "            nn.ReLU(True)  # ReLU activation\n",
    "        )\n",
    "\n",
    "        # Define layer to reshape tensor for convolutional layers\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(32, image_size//8, image_size//8))\n",
    "        \n",
    "        # Define the decoder convolutional layers\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),  # Transposed convolution to upsample\n",
    "            nn.BatchNorm2d(16),  # Batch normalization\n",
    "            nn.ReLU(True),  # ReLU activation\n",
    "            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),  # Transposed convolution to further upsample\n",
    "            nn.BatchNorm2d(8),  # Batch normalization\n",
    "            nn.ReLU(True),  # ReLU activation\n",
    "            nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1)  # Transposed convolution to get back to original channel size (1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.decoder_lin(x)  # Pass through linear layers to expand latent representation\n",
    "        x = self.unflatten(x)  # Reshape the tensor to the format expected by convolutional layers\n",
    "        x = self.decoder_conv(x)  # Pass through transposed convolutional layers to reconstruct image\n",
    "        x = torch.sigmoid(x)  # Apply sigmoid activation to output pixel values in the range [0, 1]\n",
    "        return x  # Return the reconstructed image\n",
    "\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.encoder = VariationalEncoder(latent_dims)  # Initialize the encoder\n",
    "        self.decoder = Decoder(latent_dims)  # Initialize the decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)  # Move input data to the device\n",
    "        z = self.encoder(x)  # Encode input to get latent space representation\n",
    "        return self.decoder(z)  # Decode latent space representation to reconstruct the input image\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define the dimensionality of the latent space\n",
    "d = 8\n",
    "\n",
    "# Initialize the first VAE model and load pretrained weights\n",
    "Myo_VAE = VariationalAutoencoder(latent_dims=d)\n",
    "Myo_VAE.to(device)  # Move model to the device\n",
    "Myo_VAE.load_state_dict(torch.load('Myo_VAE_' + str(image_size) + '_.pth', map_location=device))  # Load the model weights\n",
    "Myo_VAE.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Initialize the second VAE model and load pretrained weights\n",
    "LV_VAE = VariationalAutoencoder(latent_dims=d)\n",
    "LV_VAE.to(device)  # Move model to the device\n",
    "LV_VAE.load_state_dict(torch.load('LV_VAE_' + str(image_size) + '_.pth', map_location=device))  # Load the model weights\n",
    "LV_VAE.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Optional: Print model summaries (requires torchinfo package)\n",
    "# import torchinfo\n",
    "torchinfo.summary(Myo_VAE, input_size=(2, 1, 512, 512), depth=100)\n",
    "torchinfo.summary(LV_VAE, input_size=(2, 1, 512, 512), depth=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286cd90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the L2 loss function (Mean Squared Error Loss)\n",
    "L2_loss = nn.MSELoss(reduction='mean')\n",
    "\n",
    "def globalLoss(trueMask, predMask):\n",
    "    '''\n",
    "    Computes a combined loss for segmentation masks using Variational Autoencoders.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    trueMask : torch.Tensor\n",
    "        The ground truth masks with class labels.\n",
    "    predMask : torch.Tensor\n",
    "        The predicted masks from the model.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    loss_ : torch.Tensor\n",
    "        The combined loss calculated from both myocardial and LV masks.\n",
    "    '''\n",
    "    \n",
    "    # Clone the true mask to create separate masks for different regions\n",
    "    myo_trueMask = trueMask.clone()\n",
    "    LV_trueMask = trueMask.clone()\n",
    "\n",
    "    # Clone the predicted mask for thresholding and classification\n",
    "    thresholded_predMask = predMask.clone()\n",
    "\n",
    "    # Define class labels\n",
    "    background_class = torch.zeros_like(predMask)\n",
    "    myo_class = torch.ones_like(predMask)\n",
    "    lv_class = 2 * torch.ones_like(predMask)\n",
    "\n",
    "    # Apply thresholding to classify pixels\n",
    "    # Pixels < 0.98 are classified as background\n",
    "    thresholded_predMask_ = torch.where(thresholded_predMask < 0.98, background_class, thresholded_predMask)\n",
    "    \n",
    "    # Pixels > 1.0 are classified as LV class\n",
    "    thresholded_predMask_ = torch.where(thresholded_predMask_ > 1.0, lv_class, thresholded_predMask_)\n",
    "    \n",
    "    # Pixels in the range [0.98, 1.0] are classified as myocardial class\n",
    "    thresholded_predMask_ = torch.where((thresholded_predMask_ <= 1.0) & (thresholded_predMask_ >= 0.98), myo_class, thresholded_predMask_)\n",
    "\n",
    "    # Separate the predicted mask into myocardial and LV components\n",
    "    myo_predMask = thresholded_predMask_.clone()\n",
    "    LV_predMask = thresholded_predMask_.clone()\n",
    "\n",
    "    # Set LV pixels in the myocardial mask to 0 and vice versa\n",
    "    myo_predMask[myo_predMask == 2] = 0\n",
    "    myo_trueMask[myo_trueMask == 2] = 0\n",
    "\n",
    "    # Set myocardial pixels in the LV mask to 0 and normalize LV mask to be in the range [0, 1]\n",
    "    LV_predMask[LV_predMask == 1] = 0\n",
    "    LV_predMask = LV_predMask / 2\n",
    "\n",
    "    # Set myocardial pixels in the LV true mask to 0 and normalize LV true mask to be in the range [0, 1]\n",
    "    LV_trueMask[LV_trueMask == 1] = 0\n",
    "    LV_trueMask = LV_trueMask / 2\n",
    "\n",
    "    # Pass the myocardial masks through the Myo_VAE model\n",
    "    attribute_myo_true = Myo_VAE(myo_trueMask)\n",
    "    attribute_myo_pred = Myo_VAE(myo_predMask)\n",
    "\n",
    "    # Calculate the L2 loss for the myocardial masks\n",
    "    myo_L2 = L2_loss(attribute_myo_true, attribute_myo_pred)\n",
    "\n",
    "    # Pass the LV masks through the LV_VAE model\n",
    "    attribute_lv_true = LV_VAE(LV_trueMask)\n",
    "    attribute_lv_pred = LV_VAE(LV_predMask)\n",
    "\n",
    "    # Calculate the L2 loss for the LV masks\n",
    "    lv_L2 = L2_loss(attribute_lv_true, attribute_lv_pred)\n",
    "\n",
    "    # Combine the losses for myocardial and LV masks\n",
    "    loss_ = myo_L2 + lv_L2\n",
    "\n",
    "    return loss_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bb444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create real labels (1s) for the discriminator\n",
    "def label_real(size):\n",
    "    '''\n",
    "    Create a tensor of real labels (all ones) with the given size.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    size : tuple\n",
    "        The size of the tensor to be created.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    data : torch.Tensor\n",
    "        Tensor filled with ones, representing real labels.\n",
    "    '''\n",
    "    data = torch.ones(size, 1)  # Create a tensor of ones\n",
    "    return data.to(device)  # Move tensor to the specified device (GPU/CPU)\n",
    "\n",
    "# Function to create fake labels (0s) for the discriminator\n",
    "def label_fake(size):\n",
    "    '''\n",
    "    Create a tensor of fake labels (all zeros) with the given size.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    size : tuple\n",
    "        The size of the tensor to be created.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    data : torch.Tensor\n",
    "        Tensor filled with zeros, representing fake labels.\n",
    "    '''\n",
    "    data = torch.zeros(size, 1)  # Create a tensor of zeros\n",
    "    return data.to(device)  # Move tensor to the specified device (GPU/CPU)\n",
    "\n",
    "# Initialize the discriminator network\n",
    "discriminator = monai.networks.nets.Discriminator(\n",
    "    in_shape=(1, image_size, image_size),  # Input shape: grayscale images with size (img_size x img_size)\n",
    "    channels=(8, 16, 32, 64, 1),       # Number of channels for each layer\n",
    "    strides=(2, 2, 2, 2, 1),           # Strides for each convolutional layer\n",
    "    num_res_units=2,                    # Number of residual units in the network\n",
    "    kernel_size=3,                      # Kernel size for the convolutions\n",
    "    dropout=0.10,                       # Dropout rate to prevent overfitting\n",
    "    act='LEAKYRELU'                     # Activation function used: Leaky ReLU\n",
    ").to(device)  # Move the discriminator to the specified device (GPU/CPU)\n",
    "\n",
    "# Loss function for binary classification\n",
    "criterion = nn.BCELoss()\n",
    "'''\n",
    "BCELoss stands for Binary Cross Entropy Loss, used for binary classification tasks.\n",
    "In the context of GANs, it helps in distinguishing between real and fake data.\n",
    "'''\n",
    "\n",
    "# Optimizer for the discriminator\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "'''\n",
    "Adam optimizer is used to update the discriminator's weights.\n",
    "Learning rate is set to 0.0002, which controls how much to change the weights in each update.\n",
    "'''\n",
    "\n",
    "# Optional: Print the summary of the discriminator network (commented out)\n",
    "torchinfo.summary(discriminator, input_size=(2, 1, image_size, image_size), depth=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30ce8c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize variables for tracking performance metrics and losses during training\n",
    "total_epochs = num_epochs  # Total number of epochs for training\n",
    "train_epoch_losses, val_epoch_losses = [], []  # Lists to store average loss for each epoch (training and validation)\n",
    "validation_interval = 1  # Interval for running validation (every epoch in this case)\n",
    "best_dice_metric = -1  # Best Dice Similarity Coefficient (DSC) achieved so far\n",
    "best_dice_epoch = -1  # Epoch at which the best DSC was achieved\n",
    "dice_metric_history = []  # List to store DSC metrics for each validation epoch\n",
    "lowest_loss = 1e10  # Initialize to a large value to track the lowest loss\n",
    "best_dice_score = 0  # Highest DSC value achieved so far\n",
    "epoch_counter = 1  # Counter for epochs, purpose specified later\n",
    "\n",
    "train_L2_losses = []  # To store L2 loss for training data across epochs\n",
    "val_L2_losses = []  # To store L2 loss for validation data across epochs\n",
    "\n",
    "train_MI_losses = []  # To store Mutual Information (MI) loss for training data across epochs\n",
    "val_MI_losses = []  # To store Mutual Information loss for validation data across epochs\n",
    "\n",
    "# Training loop over all epochs\n",
    "for epoch in range(total_epochs):\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"Epoch {epoch + 1}/{total_epochs}\")\n",
    "    \n",
    "    model.train()  # Set model to training mode\n",
    "    epoch_loss_sum, step_count = 0, 0  # Initialize sum of losses and step counter for the epoch\n",
    "\n",
    "    # Initialize metrics for the current epoch\n",
    "    epoch_dice_sum, epoch_L2_sum, epoch_MI_sum, epoch_reg_sum = 0, 0, 0, 0\n",
    "    \n",
    "    # Iterate over batches of training data\n",
    "    for fixed_train_img, fixed_train_mask, moving_train_img, moving_train_mask in tqdm(zip(fixed_train_img_loader,\n",
    "                                                                                          fixed_train_mask_loader, \n",
    "                                                                                          moving_train_img_loader,\n",
    "                                                                                          moving_train_mask_loader)):   \n",
    "        step_count += 1  # Increment step counter\n",
    "        \n",
    "        optimizer.zero_grad()  # Reset the gradients for the optimizer\n",
    "        discriminator_optimizer.zero_grad()  # Reset the gradients for the discriminator\n",
    "\n",
    "        # Move data to the specified device (e.g., GPU)\n",
    "        fixed_train_img = fixed_train_img.to(device)\n",
    "        fixed_train_mask = fixed_train_mask.to(device)\n",
    "        moving_train_img = moving_train_img.to(device)\n",
    "        moving_train_mask = moving_train_mask.to(device)\n",
    "        \n",
    "        # Compute the deformation field using the model\n",
    "        deformation_field_train = model(torch.cat((moving_train_img, fixed_train_img), dim=1))\n",
    "        \n",
    "        # Warp images and masks using the deformation field\n",
    "        warped_image_train = warp_layer(moving_train_img, deformation_field_train)\n",
    "        warped_mask_train = warp_layer(moving_train_mask, deformation_field_train)\n",
    "\n",
    "        # Create labels for real and fake images for the discriminator\n",
    "        real_label = label_real(fixed_train_img.size(0))\n",
    "        fake_label = label_fake(fixed_train_img.size(0))\n",
    "\n",
    "        # Compute loss for real images in the discriminator\n",
    "        discriminator_output_real = discriminator(fixed_train_img)\n",
    "        loss_real = criterion(discriminator_output_real, real_label)\n",
    "        loss_real.backward()  # Backpropagate the loss\n",
    "\n",
    "        # Compute loss for fake images in the discriminator\n",
    "        discriminator_output_fake = discriminator(warped_image_train.detach())\n",
    "        loss_fake = criterion(discriminator_output_fake, fake_label)\n",
    "        loss_fake.backward()  # Backpropagate the loss\n",
    "\n",
    "        discriminator_optimizer.step()  # Update the discriminator parameters\n",
    "\n",
    "        # Average discriminator loss\n",
    "        discriminator_loss = (loss_real + loss_fake) / 2\n",
    "\n",
    "        # Compute loss for the generator (model) based on discriminator's output\n",
    "        generator_output = discriminator(warped_image_train)\n",
    "        loss_generator = criterion(generator_output, real_label)\n",
    "\n",
    "        # Compute various losses for the generator\n",
    "        image_similarity_loss = image_loss(warped_image_train, fixed_train_img)  # Image similarity loss\n",
    "        dice_loss = label_loss(fixed_train_mask, warped_mask_train)  # Dice similarity loss\n",
    "        global_loss = globalLoss(fixed_train_mask, warped_mask_train) - 1  # Global loss (modified)\n",
    "        regularization_loss = regularization(deformation_field_train)  # Regularization loss on deformation field\n",
    "\n",
    "        # Total loss for the generator combining all components\n",
    "        total_loss = (image_similarity_loss + \n",
    "                     regularization_loss + \n",
    "                     2 * global_loss + \n",
    "                     2 * dice_loss + \n",
    "                     0.0001 * loss_generator)\n",
    "        total_loss.backward()  # Backpropagate the total loss\n",
    "        optimizer.step()  # Update the generator parameters\n",
    "\n",
    "        # Accumulate losses and metrics for this step\n",
    "        epoch_loss_sum += total_loss.item()\n",
    "        epoch_dice_sum += dice_loss.item()\n",
    "        epoch_L2_sum += global_loss.item()\n",
    "        epoch_MI_sum += image_similarity_loss.item()\n",
    "        epoch_reg_sum += regularization_loss.item()\n",
    "\n",
    "        # Update dice metric using a helper function\n",
    "        dice_metric(y_pred=make_one_hot(warped_mask_train, device, C=3), \n",
    "                    y=make_one_hot(fixed_train_mask, device, C=3))\n",
    "\n",
    "    # Print the learning rate for the optimizer\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(\"Learning rate: \", param_group['lr'])\n",
    "\n",
    "    # Aggregate and average metrics and losses for the epoch\n",
    "    avg_dice_metric = dice_metric.aggregate().item()\n",
    "    dice_metric.reset()\n",
    "    avg_epoch_loss = epoch_loss_sum / step_count\n",
    "    avg_epoch_dice = epoch_dice_sum / step_count\n",
    "    avg_epoch_L2 = epoch_L2_sum / step_count\n",
    "    avg_epoch_MI = epoch_MI_sum / step_count\n",
    "    avg_epoch_reg = epoch_reg_sum / step_count\n",
    "\n",
    "    # Store L2 and MI losses for training in lists\n",
    "    train_L2_losses.append(avg_epoch_L2)\n",
    "    train_MI_losses.append(avg_epoch_MI)\n",
    "\n",
    "    # Print statistics for the current epoch\n",
    "    print(f\"Epoch {epoch + 1}: Average training total Loss: {avg_epoch_loss:.5f}\")\n",
    "    print(f\"Epoch {epoch + 1}: Average training DSC: {avg_dice_metric:.5f}\")\n",
    "    print(f\"Epoch {epoch + 1}: Average training DSC Loss: {avg_epoch_dice:.5f}\")\n",
    "    print(f\"Epoch {epoch + 1}: Average training L2 Loss: {avg_epoch_L2:.5f}\")\n",
    "    print(f\"Epoch {epoch + 1}: Average training MI Loss: {avg_epoch_MI:.5f}\")\n",
    "    print(f\"Epoch {epoch + 1}: Average training DDF Loss: {avg_epoch_reg:.5f}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Validation step every `validation_interval` epochs\n",
    "    if (epoch + 1) % validation_interval == 0 or epoch == 0:\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss_sum, val_dice_sum = 0, 0\n",
    "        val_L2_sum, val_MI_sum = 0, 0\n",
    "        val_step_count = 0\n",
    "        with torch.no_grad():  # No gradient computation during validation\n",
    "            for fixed_val_img, fixed_val_mask, moving_val_img, moving_val_mask in zip(fixed_val_img_loader, \n",
    "                                                                                      fixed_val_mask_loader,\n",
    "                                                                                      moving_val_img_loader,\n",
    "                                                                                      moving_val_mask_loader):  \n",
    "\n",
    "                # Move validation data to the specified device\n",
    "                fixed_val_img = fixed_val_img.to(device)\n",
    "                fixed_val_mask = fixed_val_mask.to(device)\n",
    "                moving_val_img = moving_val_img.to(device)\n",
    "                moving_val_mask = moving_val_mask.to(device)\n",
    "\n",
    "                # Compute deformation field for validation data\n",
    "                deformation_field_val = model(torch.cat((moving_val_img, fixed_val_img), dim=1))\n",
    "                \n",
    "                # Warp images and masks for validation\n",
    "                warped_image_val = warp_layer(moving_val_img, deformation_field_val)\n",
    "                warped_mask_val = warp_layer(moving_val_mask, deformation_field_val)\n",
    "\n",
    "                # Compute generator loss on validation data\n",
    "                real_label = label_real(warped_image_val.size(0))\n",
    "                generator_output_val = discriminator(warped_image_val)\n",
    "                loss_generator_val = criterion(generator_output_val, real_label)\n",
    "\n",
    "                # Compute various losses for validation\n",
    "                image_similarity_loss_val = image_loss(warped_image_val, fixed_val_img)\n",
    "                dice_loss_val = label_loss(fixed_val_mask, warped_mask_val)\n",
    "                global_loss_val = globalLoss(fixed_val_mask, warped_mask_val) - 1\n",
    "                regularization_loss_val = regularization(deformation_field_val)\n",
    "\n",
    "                # Total loss for validation data\n",
    "                total_val_loss = (image_similarity_loss_val + \n",
    "                                  regularization_loss_val + \n",
    "                                  2 * global_loss_val + \n",
    "                                  2 * dice_loss_val + \n",
    "                                  0.0001 * loss_generator_val)\n",
    "                \n",
    "                # Accumulate validation losses and metrics\n",
    "                val_loss_sum += total_val_loss.item()\n",
    "                val_L2_sum += global_loss_val.item()\n",
    "                val_MI_sum += image_similarity_loss_val.item()\n",
    "                val_step_count += 1\n",
    "\n",
    "                # Update dice metric for validation\n",
    "                dice_metric(y_pred=make_one_hot(warped_mask_val, device, C=3), \n",
    "                            y=make_one_hot(fixed_val_mask, device, C=3))\n",
    "                \n",
    "            # Compute average validation losses and metrics\n",
    "            avg_val_loss = val_loss_sum / val_step_count\n",
    "            avg_val_L2 = val_L2_sum / val_step_count\n",
    "            avg_val_MI = val_MI_sum / val_step_count\n",
    "            val_L2_losses.append(avg_val_L2)\n",
    "            val_MI_losses.append(avg_val_MI)\n",
    "            val_epoch_losses.append(avg_val_loss)\n",
    "            avg_dice_metric = dice_metric.aggregate().item()\n",
    "            dice_metric_history.append(avg_dice_metric)\n",
    "            dice_metric.reset()\n",
    "            \n",
    "            # Save the model if it achieves the best DSC so far\n",
    "            if avg_dice_metric > best_dice_score:\n",
    "                best_dice_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(dataset_root_dir, checkpoint_file_name + '.pth'))\n",
    "                print(f\"Validation DSC improved from {best_dice_score:2.4f} to {avg_dice_metric:2.4f}! \"\n",
    "                      f\"Saving best model as {checkpoint_file_name + '.pth'}\")\n",
    "                best_dice_score = avg_dice_metric\n",
    "                \n",
    "            # Print validation statistics\n",
    "            print(f\"\\nCurrent mean DSC: {avg_dice_metric:.4f} \\t Current validation loss: {avg_val_loss:.4f}\\n\\n\"\n",
    "                  f\"Best DSC: {best_dice_score:.4f} at epoch {best_dice_epoch}\")\n",
    "            \n",
    "# Save training and validation metrics to a CSV file\n",
    "metrics_dataframe = pd.DataFrame({\n",
    "     'Validation_L2_Loss': np.array(val_L2_losses),\n",
    "     'Training_L2_Loss': np.array(train_L2_losses),\n",
    "     'Validation_MI_Loss': np.array(val_MI_losses),\n",
    "     'Training_MI_Loss': np.array(train_MI_losses),\n",
    "})\n",
    "\n",
    "metrics_dataframe.to_csv(experiment_name + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da615d04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "886930dad8d4457679a0ca572c30287744b0c7f7c87d123d0e54902f7e45ac8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
